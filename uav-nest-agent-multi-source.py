"""
LangChain Multi-Source RAG Agent (Educational Demo)

This application demonstrates LangChain's capabilities and limitations in multi-source
RAG scenarios by comparing 3 different merge strategies.

Purpose: Compare data storage solutions between EVO Nest and DJI Dock using 6 data sources.

LangChain Limitations Demonstrated:
1. Manual management of multiple retrievers (no unified interface)
2. EnsembleRetriever cannot configure data source priority weights
3. No support for conditional queries (e.g., "query A first, then B if insufficient")
4. Lack of query routing based on question type
5. Messy logging output from multiple retrievers

Features:
- 6 independent vector stores (technical whitepapers, manuals, webpages)
- 3 merge strategies: Simple Concatenation, RRF Fusion, Priority Filtering
- Detailed logging to observe each strategy's behavior
- Clear annotation of LangChain's limitations
"""

import os
import time
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import getpass

from dotenv import load_dotenv
import lancedb

from langchain_community.document_loaders import PyPDFLoader, UnstructuredMarkdownLoader
from langchain_community.vectorstores import LanceDB
from langchain_core.documents import Document
from langchain.retrievers import EnsembleRetriever
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Try importing FastEmbed (FREE local embedder)
try:
    from langchain_community.embeddings import FastEmbedEmbeddings
    FASTEMBED_AVAILABLE = True
    print("âœ… Using FastEmbedEmbeddings (free local embedder)")
except ImportError as e:
    FASTEMBED_AVAILABLE = False
    print(f"âš ï¸  FastEmbedEmbeddings not available: {e}")
    print("   Install with: pip install fastembed")

# Try importing OpenAI embeddings as fallback
try:
    from langchain_openai import OpenAIEmbeddings
    OPENAI_EMBEDDINGS_AVAILABLE = True
except ImportError:
    OPENAI_EMBEDDINGS_AVAILABLE = False


# Load environment variables from .env file (optional)
load_dotenv()

# Load API keys (will be set interactively if not found)
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")


def get_api_key_interactive(key_name: str = "DEEPSEEK_API_KEY") -> str:
    """
    äº¤äº’å¼è·å– API Keyï¼Œä»¥æ˜Ÿå·å½¢å¼æ˜¾ç¤ºè¾“å…¥ã€‚
    
    Args:
        key_name: API Key çš„åç§°
        
    Returns:
        ç”¨æˆ·è¾“å…¥çš„ API Key
    """
    print(f"\nğŸ”‘ è¯·è¾“å…¥ä½ çš„ {key_name}")
    if key_name == "DEEPSEEK_API_KEY":
        print("   è·å–åœ°å€: https://platform.deepseek.com/")
    else:
        print("   è·å–åœ°å€: https://platform.openai.com/")
    
    api_key = getpass.getpass("   API Key (è¾“å…¥æ—¶ä¼šéšè—): ").strip()
    
    if not api_key:
        raise ValueError(f"âŒ {key_name} ä¸èƒ½ä¸ºç©º")
    
    print(f"âœ… {key_name} å·²æ¥æ”¶\n")
    return api_key


class MultiSourceRAGAgent:
    """
    Multi-Source RAG Agent (Educational Demo)
    
    This implementation is designed to showcase LangChain's limitations in multi-source scenarios:
    1. Need to manually manage 6 independent vector stores and retrievers
    2. EnsembleRetriever cannot configure data source priority weights
    3. Cannot implement conditional queries (e.g., "query A first, then B if needed")
    4. Lack of query routing (cannot auto-select data sources based on question type)
    
    These issues motivate the need for LangGraph for more complex workflows.
    
    Data Sources (6 total):
    - Priority 1 (P1): Technical whitepapers (EVO Nest, DJI Dock)
    - Priority 2 (P2): User manuals (EVO Nest, DJI Dock)
    - Priority 3 (P3): Official webpages (EVO Nest, DJI Dock)
    """
    
    # âš ï¸ LangChain Limitation #3: Need to manually define and manage data source configuration
    # No built-in DataSourceManager or similar abstraction
    DATA_SOURCES = [
        {
            "id": "evo_nest_whitepaper",
            "name": "EVO NestæŠ€æœ¯ç™½çš®ä¹¦",
            "file_path": "evo-nest-data-storage-spec.md",
            "file_type": "markdown",
            "priority": 1,  # Highest priority
            "product": "evo_nest",
            "source_type": "whitepaper",
        },
        {
            "id": "dji_dock_whitepaper",
            "name": "DJI DockæŠ€æœ¯ç™½çš®ä¹¦",
            "file_path": "dji-dock-data-storage-spec.md",
            "file_type": "markdown",
            "priority": 1,  # Highest priority
            "product": "dji_dock",
            "source_type": "whitepaper",
        },
        {
            "id": "evo_nest_manual",
            "name": "EVO Nestç”¨æˆ·æ‰‹å†Œ",
            "file_path": "EN_EVO-Nest-Kit-User-Manual_V1.0.1.pdf",
            "file_type": "pdf",
            "priority": 2,  # Medium priority
            "product": "evo_nest",
            "source_type": "manual",
        },
        {
            "id": "dji_dock_manual",
            "name": "DJI Dockç”¨æˆ·æ‰‹å†Œ",
            "file_path": "M30_Series_Dock_Bundle_User_Manual_v1.8_CHS.pdf",
            "file_type": "pdf",
            "priority": 2,  # Medium priority
            "product": "dji_dock",
            "source_type": "manual",
            "filter_keywords": ["å­˜å‚¨", "æ•°æ®ç®¡ç†", "å†…å­˜", "SD", "æ•°æ®ä¼ è¾“", "å¤‡ä»½", "å®¹é‡"],
        },
        {
            "id": "evo_nest_webpage",
            "name": "EVO Nestå®˜ç½‘ä»‹ç»",
            "file_path": "evo-nest-official-webpage.md",
            "file_type": "markdown",
            "priority": 3,  # Lowest priority
            "product": "evo_nest",
            "source_type": "webpage",
        },
        {
            "id": "dji_dock_webpage",
            "name": "DJI Dockå®˜ç½‘ä»‹ç»",
            "file_path": "dji-dock-official-webpage.md",
            "file_type": "markdown",
            "priority": 3,  # Lowest priority
            "product": "dji_dock",
            "source_type": "webpage",
        },
    ]
    
    def __init__(
        self,
        vector_db_path: str = "./tmp/lancedb",
        use_fastembed: bool = True,
        deepseek_api_key: Optional[str] = None,
        openai_api_key: Optional[str] = None,
    ):
        """
        Initialize the Multi-Source RAG Agent.
        
        Args:
            vector_db_path: Base path for LanceDB vector databases
            use_fastembed: Whether to use FastEmbed (free) or OpenAI embeddings
            deepseek_api_key: DeepSeek API Key (optional, will prompt if not provided)
            openai_api_key: OpenAI API Key (optional, only needed if not using FastEmbed)
        """
        self.vector_db_path = vector_db_path
        
        # Get DeepSeek API Key
        self.deepseek_api_key = deepseek_api_key or DEEPSEEK_API_KEY
        if not self.deepseek_api_key:
            self.deepseek_api_key = get_api_key_interactive("DEEPSEEK_API_KEY")
        
        # Get OpenAI API Key if needed
        self.openai_api_key = openai_api_key or OPENAI_API_KEY
        if not use_fastembed and not self.openai_api_key:
            self.openai_api_key = get_api_key_interactive("OPENAI_API_KEY")
        
        # Initialize embeddings
        self.embeddings = self._initialize_embeddings(use_fastembed)
        
        # Initialize LLM (DeepSeek)
        self.llm = self._initialize_llm()
        
        # Storage for vector stores and retrievers
        # âš ï¸ LangChain Limitation #3: Need to manually track multiple vector stores and retrievers
        # No unified management interface
        self.vector_stores: Dict[str, LanceDB] = {}
        self.retrievers: Dict[str, any] = {}
        
        # Group retrievers by priority for strategy 4
        self.retrievers_by_priority: Dict[int, List[any]] = {1: [], 2: [], 3: []}
        
        # RAG chains for different strategies
        self.rag_chains: Dict[str, any] = {}
    
    @staticmethod
    def filter_documents_by_keywords(
        documents: List[Document],
        keywords: List[str]
    ) -> List[Document]:
        """
        Filter documents by keywords (for large PDF files).
        
        Args:
            documents: List of documents to filter
            keywords: List of keywords to search for
            
        Returns:
            Filtered list of documents containing at least one keyword
        """
        filtered = []
        for doc in documents:
            content_lower = doc.page_content.lower()
            # Check if any keyword exists in the content
            if any(keyword.lower() in content_lower for keyword in keywords):
                filtered.append(doc)
        return filtered
    
    def _load_markdown_file(self, file_path: str, source_config: dict) -> List[Document]:
        """
        Load a Markdown file and add metadata.
        
        Args:
            file_path: Path to the Markdown file
            source_config: Configuration dict for this data source
            
        Returns:
            List of documents with metadata
        """
        if not Path(file_path).exists():
            raise FileNotFoundError(f"âŒ File not found: {file_path}")
        
        print(f"  ğŸ“„ Loading {source_config['name']} from {file_path}...")
        loader = UnstructuredMarkdownLoader(file_path)
        documents = loader.load()
        
        # Add metadata
        for doc in documents:
            doc.metadata.update({
                "source_id": source_config["id"],
                "source_name": source_config["name"],
                "source_type": source_config["source_type"],
                "priority": source_config["priority"],
                "product": source_config["product"],
                "source_file": file_path,
            })
        
        print(f"  âœ… Loaded {len(documents)} documents")
        return documents
    
    def _load_pdf_file(
        self,
        file_path: str,
        source_config: dict,
        filter_keywords: Optional[List[str]] = None
    ) -> List[Document]:
        """
        Load a PDF file and optionally filter by keywords.
        
        Args:
            file_path: Path to the PDF file
            source_config: Configuration dict for this data source
            filter_keywords: Optional list of keywords to filter content
            
        Returns:
            List of documents with metadata
        """
        if not Path(file_path).exists():
            raise FileNotFoundError(f"âŒ File not found: {file_path}")
        
        print(f"  ğŸ“„ Loading {source_config['name']} from {file_path}...")
        loader = PyPDFLoader(file_path)
        
        try:
            documents = loader.load()
            print(f"  âœ… Loaded {len(documents)} pages")
            
            # Filter by keywords if specified
            if filter_keywords:
                print(f"  ğŸ” Filtering by keywords: {', '.join(filter_keywords)}")
                documents = self.filter_documents_by_keywords(documents, filter_keywords)
                print(f"  âœ… Filtered to {len(documents)} relevant pages")
            
            # Add metadata
            for doc in documents:
                doc.metadata.update({
                    "source_id": source_config["id"],
                    "source_name": source_config["name"],
                    "source_type": source_config["source_type"],
                    "priority": source_config["priority"],
                    "product": source_config["product"],
                    "source_file": file_path,
                })
            
            return documents
            
        except Exception as e:
            print(f"  âš ï¸  Error loading PDF with PyPDFLoader: {e}")
            print(f"  ğŸ’¡ Tip: If this is a binary PDF, try installing: pip install unstructured")
            raise
    
    def load_all_data_sources(self, force_reload: bool = False) -> Dict[str, List[Document]]:
        """
        Load all 6 data sources and return documents grouped by source ID.
        
        Args:
            force_reload: If True, reload even if already loaded
            
        Returns:
            Dictionary mapping source_id to list of documents
        """
        print("\n" + "="*60)
        print("ğŸ“š Loading All Data Sources")
        print("="*60)
        
        all_documents = {}
        
        # âš ï¸ LangChain Limitation #3: Need to manually iterate and load each data source
        # No built-in batch loading or data source manager
        for source_config in self.DATA_SOURCES:
            source_id = source_config["id"]
            file_path = source_config["file_path"]
            file_type = source_config["file_type"]
            
            print(f"\n[{source_config['priority']}/P{source_config['priority']}] {source_config['name']}")
            
            try:
                if file_type == "markdown":
                    documents = self._load_markdown_file(file_path, source_config)
                elif file_type == "pdf":
                    filter_keywords = source_config.get("filter_keywords")
                    documents = self._load_pdf_file(file_path, source_config, filter_keywords)
                else:
                    raise ValueError(f"Unsupported file type: {file_type}")
                
                all_documents[source_id] = documents
                
            except Exception as e:
                print(f"  âŒ Error loading {source_id}: {e}")
                # Store empty list for failed sources
                all_documents[source_id] = []
        
        print("\n" + "="*60)
        print(f"âœ… Successfully loaded {len([d for d in all_documents.values() if d])} data sources")
        print("="*60)
        
        return all_documents
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """
        Split documents into chunks using RecursiveCharacterTextSplitter.
        
        Args:
            documents: List of documents to split
            
        Returns:
            List of chunked documents
        """
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,  # Smaller than single-source RAG to fit more sources
            chunk_overlap=200,
            length_function=len,
            separators=["\n\n", "\n", "ã€‚", "ï¼›", " ", ""]  # Support Chinese and English
        )
        
        chunks = text_splitter.split_documents(documents)
        return chunks
    
    def create_vector_stores(
        self,
        all_documents: Dict[str, List[Document]],
        force_reload: bool = False
    ) -> Dict[str, LanceDB]:
        """
        Create vector stores for all data sources.
        
        Args:
            all_documents: Dictionary mapping source_id to documents
            force_reload: If True, recreate vector stores even if they exist
            
        Returns:
            Dictionary mapping source_id to LanceDB vector store
        """
        print("\n" + "="*60)
        print("ğŸ—„ï¸  Creating Vector Stores")
        print("="*60)
        
        db = lancedb.connect(self.vector_db_path)
        vector_stores = {}
        
        # âš ï¸ LangChain Limitation #3: Need to manually create each vector store
        # No batch processing or unified vector store manager
        for source_id, documents in all_documents.items():
            if not documents:
                print(f"\nâš ï¸  Skipping {source_id} (no documents loaded)")
                continue
            
            source_config = next(s for s in self.DATA_SOURCES if s["id"] == source_id)
            table_name = source_id
            
            print(f"\n[P{source_config['priority']}] {source_config['name']}")
            print(f"  ğŸ“Š Table: {table_name}")
            
            # Check if table already exists
            table_exists = table_name in db.table_names()
            
            if table_exists and not force_reload:
                try:
                    table = db.open_table(table_name)
                    row_count = table.count_rows()
                    if row_count > 0:
                        print(f"  ğŸ“‚ Loading existing table ({row_count} rows)")
                        vector_store = LanceDB(
                            connection=db,
                            embedding=self.embeddings,
                            table_name=table_name,
                        )
                        vector_stores[source_id] = vector_store
                        continue
                except Exception as e:
                    print(f"  âš ï¸  Error loading existing table: {e}")
            
            # Split documents into chunks
            print(f"  âœ‚ï¸  Splitting {len(documents)} documents into chunks...")
            chunks = self.split_documents(documents)
            print(f"  âœ… Created {len(chunks)} chunks")
            
            # Create vector store
            print(f"  ğŸ”§ Creating vector store (this may take a moment)...")
            vector_store = LanceDB.from_documents(
                documents=chunks,
                embedding=self.embeddings,
                connection=db,
                table_name=table_name,
            )
            vector_stores[source_id] = vector_store
            print(f"  âœ… Vector store created")
        
        print("\n" + "="*60)
        print(f"âœ… Successfully created {len(vector_stores)} vector stores")
        print("="*60)
        
        self.vector_stores = vector_stores
        return vector_stores
    
    def create_retrievers(self, k: int = 5) -> Dict[str, any]:
        """
        Create retrievers for all vector stores.
        
        Args:
            k: Number of documents to retrieve per source
            
        Returns:
            Dictionary mapping source_id to retriever
        """
        print("\n" + "="*60)
        print("ğŸ” Creating Retrievers")
        print("="*60)
        
        retrievers = {}
        
        # âš ï¸ LangChain Limitation #3: Need to manually create each retriever
        # No unified retriever manager or batch creation
        for source_id, vector_store in self.vector_stores.items():
            source_config = next(s for s in self.DATA_SOURCES if s["id"] == source_id)
            
            retriever = vector_store.as_retriever(
                search_type="similarity",
                search_kwargs={"k": k}
            )
            
            retrievers[source_id] = retriever
            
            # Group by priority for strategy 4
            priority = source_config["priority"]
            self.retrievers_by_priority[priority].append(retriever)
            
            print(f"  âœ… [P{priority}] {source_config['name']}: retriever created (k={k})")
        
        print("\n" + "="*60)
        print(f"âœ… Created {len(retrievers)} retrievers")
        print(f"   P1 (Whitepapers): {len(self.retrievers_by_priority[1])} retrievers")
        print(f"   P2 (Manuals): {len(self.retrievers_by_priority[2])} retrievers")
        print(f"   P3 (Webpages): {len(self.retrievers_by_priority[3])} retrievers")
        print("="*60)
        
        self.retrievers = retrievers
        return retrievers
    
    def retrieve_strategy_simple_concat(self, query: str) -> Tuple[List[Document], Dict]:
        """
        Strategy 1: Simple Concatenation
        
        Retrieve from all sources and concatenate results in priority order.
        
        Args:
            query: Query string
            
        Returns:
            Tuple of (documents list, statistics dict)
        """
        print("\n" + "â”€"*60)
        print("ğŸ“Š ç­–ç•¥1: ç®€å•æ‹¼æ¥ (Simple Concatenation)")
        print("â”€"*60)
        
        all_docs = []
        stats = {
            "P1": {"count": 0, "sources": []},
            "P2": {"count": 0, "sources": []},
            "P3": {"count": 0, "sources": []},
        }
        
        print("\nğŸ” æ£€ç´¢é˜¶æ®µï¼š")
        
        # Retrieve from P1, P2, P3 in order
        for priority in [1, 2, 3]:
            priority_docs = []
            
            for retriever in self.retrievers_by_priority[priority]:
                docs = retriever.get_relevant_documents(query)
                priority_docs.extend(docs)
            
            # Get source names for this priority
            priority_sources = set()
            for doc in priority_docs:
                if "source_name" in doc.metadata:
                    priority_sources.add(doc.metadata["source_name"])
            
            stats[f"P{priority}"]["count"] = len(priority_docs)
            stats[f"P{priority}"]["sources"] = list(priority_sources)
            
            all_docs.extend(priority_docs)
            
            priority_label = {1: "æŠ€æœ¯ç™½çš®ä¹¦", 2: "ç”¨æˆ·æ‰‹å†Œ", 3: "å®˜ç½‘ä»‹ç»"}[priority]
            print(f"  âœ… P{priority} ({priority_label}): {len(priority_docs)} chunks")
            for source in sorted(priority_sources):
                source_count = len([d for d in priority_docs if d.metadata.get("source_name") == source])
                print(f"     - {source}: {source_count} chunks")
        
        print(f"  ğŸ“¦ æ€»è®¡: {len(all_docs)} chunks")
        
        return all_docs, stats
    
    def retrieve_strategy_rrf(self, query: str) -> Tuple[List[Document], Dict]:
        """
        Strategy 3: RRF Fusion (using LangChain's EnsembleRetriever)
        
        Uses Reciprocal Rank Fusion to combine results from all retrievers.
        
        âš ï¸ LangChain Limitation #1: EnsembleRetriever cannot configure data source priority weights.
        All retrievers are treated equally, cannot reflect quality differences.
        
        Args:
            query: Query string
            
        Returns:
            Tuple of (documents list, statistics dict)
        """
        print("\n" + "â”€"*60)
        print("ğŸ“Š ç­–ç•¥3: RRFèåˆ (RRF Fusion)")
        print("â”€"*60)
        print("âš ï¸  LangChainçš„EnsembleRetrieveræ— æ³•é…ç½®æ•°æ®æºä¼˜å…ˆçº§æƒé‡")
        
        # âš ï¸ LangChain Limitation #1: Cannot set weights for different data sources
        # All retrievers are treated equally in EnsembleRetriever
        all_retrievers = list(self.retrievers.values())
        
        # Create ensemble retriever with equal weights
        ensemble_retriever = EnsembleRetriever(
            retrievers=all_retrievers,
            weights=[1.0] * len(all_retrievers)  # Equal weights, cannot be customized by priority
        )
        
        print("\nğŸ” æ£€ç´¢é˜¶æ®µï¼ˆä½¿ç”¨LangChainçš„EnsembleRetrieverï¼‰ï¼š")
        docs = ensemble_retriever.get_relevant_documents(query)
        
        # Analyze distribution by priority
        stats = {
            "P1": {"count": 0, "sources": set()},
            "P2": {"count": 0, "sources": set()},
            "P3": {"count": 0, "sources": set()},
        }
        
        for doc in docs:
            priority = doc.metadata.get("priority", 0)
            source_name = doc.metadata.get("source_name", "Unknown")
            if priority in [1, 2, 3]:
                stats[f"P{priority}"]["count"] += 1
                stats[f"P{priority}"]["sources"].add(source_name)
        
        # Convert sets to lists for stats
        for p in ["P1", "P2", "P3"]:
            stats[p]["sources"] = list(stats[p]["sources"])
        
        print(f"  âœ… èåˆåè¿”å›: {len(docs)} chunks (LangChainè‡ªåŠ¨å»é‡å’Œé‡æ’åº)")
        print(f"  ğŸ“Š æ•°æ®æºåˆ†å¸ƒ:")
        print(f"     - P1: {stats['P1']['count']} chunks")
        print(f"     - P2: {stats['P2']['count']} chunks")
        print(f"     - P3: {stats['P3']['count']} chunks")
        
        return docs, stats
    
    def retrieve_strategy_priority_filter(
        self,
        query: str,
        threshold: int = 8
    ) -> Tuple[List[Document], Dict]:
        """
        Strategy 4: Priority Filtering (Custom Implementation)
        
        Query high-priority sources first, only query lower priorities if insufficient results.
        
        âš ï¸ LangChain Limitation #2: Standard RAG chains do not support conditional branching logic.
        This "query A first, then B if needed" pattern requires complete custom implementation.
        Cannot use LangChain's Chain abstraction, must manually write control flow.
        
        Args:
            query: Query string
            threshold: Minimum number of documents required
            
        Returns:
            Tuple of (documents list, statistics dict)
        """
        print("\n" + "â”€"*60)
        print("ğŸ“Š ç­–ç•¥4: ä¼˜å…ˆçº§è¿‡æ»¤ (Priority Filtering)")
        print("â”€"*60)
        print("âš ï¸  LangChainæ ‡å‡†RAGé“¾ä¸æ”¯æŒæ¡ä»¶åˆ†æ”¯é€»è¾‘")
        print(f"   éœ€è¦å®Œå…¨è‡ªå®šä¹‰å®ç°ï¼ˆé˜ˆå€¼={threshold}ï¼‰\n")
        
        all_docs = []
        stats = {
            "stages": [],
            "P1": {"count": 0, "sources": []},
            "P2": {"count": 0, "sources": []},
            "P3": {"count": 0, "sources": []},
        }
        
        # âš ï¸ LangChain Limitation #2: Need to manually implement conditional query logic
        # Cannot use standard Chain abstraction for this pattern
        
        # Stage 1: Query P1 (highest priority)
        print("ğŸ” é˜¶æ®µ1: æŸ¥è¯¢P1 (æŠ€æœ¯ç™½çš®ä¹¦)...")
        p1_docs = []
        for retriever in self.retrievers_by_priority[1]:
            docs = retriever.get_relevant_documents(query)
            p1_docs.extend(docs)
        
        p1_sources = set(doc.metadata.get("source_name") for doc in p1_docs if "source_name" in doc.metadata)
        stats["P1"]["count"] = len(p1_docs)
        stats["P1"]["sources"] = list(p1_sources)
        
        print(f"   â†’ æ£€ç´¢åˆ° {len(p1_docs)} chunks")
        
        if len(p1_docs) >= threshold:
            print(f"   âœ… è¶…è¿‡é˜ˆå€¼ ({threshold})ï¼Œä½¿ç”¨P1ç»“æœ")
            stats["stages"].append("P1 only (sufficient)")
            all_docs = p1_docs
            print(f"   â­ï¸  è·³è¿‡P2å’ŒP3æŸ¥è¯¢ï¼ˆP1ç»“æœå·²è¶³å¤Ÿï¼‰")
        else:
            print(f"   âš ï¸  ä½äºé˜ˆå€¼ ({threshold})ï¼Œç»§ç»­æŸ¥è¯¢P2...")
            
            # Stage 2: Query P2 (medium priority)
            print("\nğŸ” é˜¶æ®µ2: æŸ¥è¯¢P2 (ç”¨æˆ·æ‰‹å†Œ)...")
            p2_docs = []
            for retriever in self.retrievers_by_priority[2]:
                docs = retriever.get_relevant_documents(query)
                p2_docs.extend(docs)
            
            p2_sources = set(doc.metadata.get("source_name") for doc in p2_docs if "source_name" in doc.metadata)
            stats["P2"]["count"] = len(p2_docs)
            stats["P2"]["sources"] = list(p2_sources)
            
            print(f"   â†’ æ£€ç´¢åˆ° {len(p2_docs)} chunks")
            
            if len(p1_docs) + len(p2_docs) >= threshold:
                print(f"   âœ… P1+P2 è¶…è¿‡é˜ˆå€¼ ({threshold})ï¼Œä½¿ç”¨P1+P2ç»“æœ")
                stats["stages"].append("P1+P2 (sufficient)")
                all_docs = p1_docs + p2_docs
                print(f"   â­ï¸  è·³è¿‡P3æŸ¥è¯¢")
            else:
                print(f"   âš ï¸  ä»ä½äºé˜ˆå€¼ ({threshold})ï¼ŒæŸ¥è¯¢P3...")
                
                # Stage 3: Query P3 (lowest priority)
                print("\nğŸ” é˜¶æ®µ3: æŸ¥è¯¢P3 (å®˜ç½‘ä»‹ç»)...")
                p3_docs = []
                for retriever in self.retrievers_by_priority[3]:
                    docs = retriever.get_relevant_documents(query)
                    p3_docs.extend(docs)
                
                p3_sources = set(doc.metadata.get("source_name") for doc in p3_docs if "source_name" in doc.metadata)
                stats["P3"]["count"] = len(p3_docs)
                stats["P3"]["sources"] = list(p3_sources)
                
                print(f"   â†’ æ£€ç´¢åˆ° {len(p3_docs)} chunks")
                print(f"   âœ… ä½¿ç”¨å…¨éƒ¨ç»“æœ (P1+P2+P3)")
                stats["stages"].append("P1+P2+P3 (all sources)")
                all_docs = p1_docs + p2_docs + p3_docs
        
        print(f"\nğŸ“¦ æœ€ç»ˆä½¿ç”¨: {len(all_docs)} chunks")
        
        return all_docs, stats
    
    @staticmethod
    def format_docs_with_source(docs: List[Document]) -> str:
        """
        Format documents with source information for the prompt.
        
        Args:
            docs: List of documents to format
            
        Returns:
            Formatted string with source annotations
        """
        # Group by priority
        docs_by_priority = {1: [], 2: [], 3: []}
        for doc in docs:
            priority = doc.metadata.get("priority", 3)
            if priority in docs_by_priority:
                docs_by_priority[priority].append(doc)
        
        formatted_parts = []
        
        priority_labels = {
            1: "P1: æŠ€æœ¯ç™½çš®ä¹¦ (æœ€é«˜ä¼˜å…ˆçº§)",
            2: "P2: ç”¨æˆ·æ‰‹å†Œ (ä¸­ä¼˜å…ˆçº§)",
            3: "P3: å®˜ç½‘ä»‹ç» (ä½ä¼˜å…ˆçº§)"
        }
        
        for priority in [1, 2, 3]:
            priority_docs = docs_by_priority[priority]
            if not priority_docs:
                continue
            
            formatted_parts.append(f"\n{'='*60}")
            formatted_parts.append(f"{priority_labels[priority]}")
            formatted_parts.append(f"{'='*60}\n")
            
            for i, doc in enumerate(priority_docs, 1):
                source_name = doc.metadata.get("source_name", "Unknown")
                product = doc.metadata.get("product", "")
                page = doc.metadata.get("page", "")
                
                header = f"[{source_name}"
                if page:
                    header += f", Page {page + 1}"
                header += "]"
                
                formatted_parts.append(header)
                formatted_parts.append(doc.page_content)
                formatted_parts.append("")  # Empty line
        
        return "\n".join(formatted_parts)
    
    def setup_rag_chains(self):
        """
        Setup RAG chains for all 3 strategies.
        """
        # Common prompt template
        template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ— äººæœºäº§å“å¯¹æ¯”åˆ†æä¸“å®¶ï¼Œæ“…é•¿æ¯”è¾ƒEVO Nestå’ŒDJI Dockçš„æŠ€æœ¯æ–¹æ¡ˆã€‚

è¯·æ ¹æ®ä»¥ä¸‹äº§å“æ–‡æ¡£æ¥å›ç­”é—®é¢˜ã€‚æ–‡æ¡£æŒ‰ä¼˜å…ˆçº§æ’åºï¼š
- P1 (æŠ€æœ¯ç™½çš®ä¹¦): æœ€æƒå¨çš„æŠ€æœ¯è§„æ ¼ä¿¡æ¯
- P2 (ç”¨æˆ·æ‰‹å†Œ): è¯¦ç»†çš„æ“ä½œå’Œé…ç½®è¯´æ˜  
- P3 (å®˜ç½‘ä»‹ç»): äº§å“æ¦‚è¿°å’Œè¥é”€ä¿¡æ¯

äº§å“æ–‡æ¡£ï¼š
{context}

é—®é¢˜ï¼š{question}

è¯·æä¾›ç»“æ„åŒ–çš„å¯¹æ¯”åˆ†æï¼ŒåŒ…æ‹¬ï¼š
1. EVO Nestçš„æ–¹æ¡ˆç‰¹ç‚¹ï¼ˆå¼•ç”¨å…·ä½“æ•°æ®ï¼‰
2. DJI Dockçš„æ–¹æ¡ˆç‰¹ç‚¹ï¼ˆå¼•ç”¨å…·ä½“æ•°æ®ï¼‰
3. ä¸¤è€…çš„å…³é”®å·®å¼‚æ€»ç»“
4. æ ‡æ³¨å¼•ç”¨æ¥æºï¼ˆå¦‚ï¼š[EVO NestæŠ€æœ¯ç™½çš®ä¹¦, Page X]ï¼‰

å›ç­”ï¼š"""
        
        prompt = ChatPromptTemplate.from_template(template)
        output_parser = StrOutputParser()
        
        # Note: We cannot create RAG chains here because they need query-specific context
        # Store the prompt and parser for later use
        self.prompt = prompt
        self.output_parser = output_parser
        
        print("\nâœ… RAG chains configured (prompt template and output parser ready)")
    
    def compare_strategies(self, question: str) -> Dict:
        """
        Compare all 3 strategies on the same question.
        
        Args:
            question: Question to ask
            
        Returns:
            Dictionary with results from all strategies
        """
        print("\n" + "="*60)
        print("ğŸ¯ å¤šæºRAGç­–ç•¥å¯¹æ¯”å®éªŒ")
        print("="*60)
        print(f"\né—®é¢˜ï¼š{question}\n")
        
        results = {}
        
        # Strategy 1: Simple Concatenation
        print("\n" + "="*60)
        print("ç­–ç•¥1: ç®€å•æ‹¼æ¥")
        print("="*60)
        start_time = time.time()
        docs1, stats1 = self.retrieve_strategy_simple_concat(question)
        context1 = self.format_docs_with_source(docs1)
        
        print("\nğŸ’¬ ç”Ÿæˆç­”æ¡ˆä¸­...")
        answer1 = (self.prompt | self.llm | self.output_parser).invoke({
            "context": context1,
            "question": question
        })
        elapsed1 = time.time() - start_time
        
        results["strategy1"] = {
            "name": "ç®€å•æ‹¼æ¥",
            "docs": docs1,
            "stats": stats1,
            "answer": answer1,
            "elapsed": elapsed1,
        }
        
        print(f"\nâ±ï¸  è€—æ—¶: {elapsed1:.1f}ç§’")
        
        # Strategy 3: RRF Fusion
        print("\n" + "="*60)
        print("ç­–ç•¥3: RRFèåˆ")
        print("="*60)
        start_time = time.time()
        docs3, stats3 = self.retrieve_strategy_rrf(question)
        context3 = self.format_docs_with_source(docs3)
        
        print("\nğŸ’¬ ç”Ÿæˆç­”æ¡ˆä¸­...")
        answer3 = (self.prompt | self.llm | self.output_parser).invoke({
            "context": context3,
            "question": question
        })
        elapsed3 = time.time() - start_time
        
        results["strategy3"] = {
            "name": "RRFèåˆ",
            "docs": docs3,
            "stats": stats3,
            "answer": answer3,
            "elapsed": elapsed3,
        }
        
        print(f"\nâ±ï¸  è€—æ—¶: {elapsed3:.1f}ç§’")
        
        # Strategy 4: Priority Filtering
        print("\n" + "="*60)
        print("ç­–ç•¥4: ä¼˜å…ˆçº§è¿‡æ»¤")
        print("="*60)
        start_time = time.time()
        docs4, stats4 = self.retrieve_strategy_priority_filter(question, threshold=8)
        context4 = self.format_docs_with_source(docs4)
        
        print("\nğŸ’¬ ç”Ÿæˆç­”æ¡ˆä¸­...")
        answer4 = (self.prompt | self.llm | self.output_parser).invoke({
            "context": context4,
            "question": question
        })
        elapsed4 = time.time() - start_time
        
        results["strategy4"] = {
            "name": "ä¼˜å…ˆçº§è¿‡æ»¤",
            "docs": docs4,
            "stats": stats4,
            "answer": answer4,
            "elapsed": elapsed4,
        }
        
        print(f"\nâ±ï¸  è€—æ—¶: {elapsed4:.1f}ç§’")
        
        return results
    
    def print_comparison_report(self, results: Dict):
        """
        Print a comprehensive comparison report.
        
        Args:
            results: Results from compare_strategies()
        """
        print("\n" + "="*60)
        print("ğŸ“ˆ ç­–ç•¥å¯¹æ¯”æ€»ç»“")
        print("="*60)
        
        # Print answers
        for strategy_key in ["strategy1", "strategy3", "strategy4"]:
            result = results[strategy_key]
            print(f"\n{'â”€'*60}")
            print(f"ã€{result['name']}ã€‘çš„å›ç­”ï¼š")
            print(f"{'â”€'*60}")
            print(result["answer"])
            print(f"\nâ±ï¸  ç”Ÿæˆè€—æ—¶: {result['elapsed']:.1f}ç§’")
            print(f"ğŸ“¦ ä½¿ç”¨chunksæ•°: {len(result['docs'])}ä¸ª")
        
        # Comparison table
        print("\n" + "="*60)
        print("ğŸ“Š ç­–ç•¥å¯¹æ¯”æŒ‡æ ‡")
        print("="*60)
        
        print(f"\n{'æŒ‡æ ‡':<20} {'ç­–ç•¥1':<15} {'ç­–ç•¥3':<15} {'ç­–ç•¥4':<15}")
        print("â”€" * 70)
        
        # Chunks count
        print(f"{'æ£€ç´¢chunksæ•°':<20} "
              f"{len(results['strategy1']['docs']):<15} "
              f"{len(results['strategy3']['docs']):<15} "
              f"{len(results['strategy4']['docs']):<15}")
        
        # P1 ratio
        def calc_p1_ratio(docs):
            p1_count = len([d for d in docs if d.metadata.get("priority") == 1])
            return f"{p1_count}/{len(docs)} ({100*p1_count/len(docs):.0f}%)" if docs else "0/0"
        
        print(f"{'é«˜ä¼˜å…ˆçº§chunks':<20} "
              f"{calc_p1_ratio(results['strategy1']['docs']):<15} "
              f"{calc_p1_ratio(results['strategy3']['docs']):<15} "
              f"{calc_p1_ratio(results['strategy4']['docs']):<15}")
        
        # Time
        print(f"{'ç”Ÿæˆè€—æ—¶(ç§’)':<20} "
              f"{results['strategy1']['elapsed']:<15.1f} "
              f"{results['strategy3']['elapsed']:<15.1f} "
              f"{results['strategy4']['elapsed']:<15.1f}")
        
        # LangChain Limitations Summary
        print("\n" + "="*60)
        print("âš ï¸  LangChainåœ¨å¤šæºRAGåœºæ™¯ä¸‹çš„å±€é™æ€§")
        print("="*60)
        
        limitations = [
            "1. âŒ éœ€è¦æ‰‹åŠ¨ç®¡ç†å¤šä¸ªæ£€ç´¢å™¨ï¼Œç¼ºä¹ç»Ÿä¸€çš„ç®¡ç†æ¥å£",
            "   - å¿…é¡»åˆ†åˆ«åˆ›å»º6ä¸ªvector storeå’Œ6ä¸ªretriever",
            "   - æ²¡æœ‰DataSourceManageræˆ–ç±»ä¼¼çš„ç»Ÿä¸€æŠ½è±¡",
            "",
            "2. âŒ EnsembleRetrieveræ— æ³•é…ç½®æ•°æ®æºä¼˜å…ˆçº§æƒé‡",
            "   - æ‰€æœ‰æ•°æ®æºè¢«å¹³ç­‰å¯¹å¾…",
            "   - æ— æ³•ä½“ç°\"æŠ€æœ¯ç™½çš®ä¹¦>ç”¨æˆ·æ‰‹å†Œ>å®˜ç½‘\"çš„è´¨é‡å·®å¼‚",
            "",
            "3. âŒ æ— æ³•å®ç°æ¡ä»¶æŸ¥è¯¢é€»è¾‘ï¼ˆå¦‚\"å…ˆæŸ¥Aï¼Œä¸å¤Ÿå†æŸ¥B\"ï¼‰",
            "   - æ ‡å‡†RAG Chainä¸æ”¯æŒæ¡ä»¶åˆ†æ”¯",
            "   - ç­–ç•¥4éœ€è¦å®Œå…¨è‡ªå®šä¹‰å®ç°100+è¡Œä»£ç ",
            "   - æ— æ³•ä½¿ç”¨LangChainçš„ChainæŠ½è±¡",
            "",
            "4. âŒ ç¼ºä¹æŸ¥è¯¢è·¯ç”±åŠŸèƒ½",
            "   - æ— æ³•æ ¹æ®é—®é¢˜ç±»å‹è‡ªåŠ¨é€‰æ‹©æ•°æ®æº",
            "   - ä¸èƒ½å®ç°\"æŠ€æœ¯é—®é¢˜â†’ç™½çš®ä¹¦ï¼Œæ“ä½œé—®é¢˜â†’æ‰‹å†Œ\"",
            "",
            "5. âŒ å¤šæ£€ç´¢å™¨çš„æ—¥å¿—è¾“å‡ºæ··ä¹±ï¼Œéš¾ä»¥è¿½è¸ª",
            "   - éœ€è¦æ‰‹åŠ¨æ·»åŠ å¤§é‡printè¯­å¥æ‰èƒ½çœ‹æ¸…è¿‡ç¨‹",
            "   - æ²¡æœ‰å†…ç½®çš„å¯è§‚æµ‹æ€§å·¥å…·",
        ]
        
        for limitation in limitations:
            print(limitation)
        
        print("\n" + "="*60)
        print("ğŸ’¡ è¿™äº›é—®é¢˜æ­£æ˜¯LangGraphè¦è§£å†³çš„ï¼")
        print("="*60)
        print("\nLangGraphæä¾›ï¼š")
        print("  âœ… å›¾ç»“æ„çš„å·¥ä½œæµç¼–æ’ï¼ˆæ”¯æŒæ¡ä»¶åˆ†æ”¯ã€å¾ªç¯ï¼‰")
        print("  âœ… çŠ¶æ€ç®¡ç†å’Œæ¡ä»¶è·¯ç”±")
        print("  âœ… å¤šAgentåä½œèƒ½åŠ›")
        print("  âœ… å†…ç½®çš„å¯è§‚æµ‹æ€§å’Œè°ƒè¯•å·¥å…·")
        print("\nä¸‹ä¸€æ­¥å¯ä»¥æ¢ç´¢ä½¿ç”¨LangGraphé‡æ„å¤šæºRAGç³»ç»Ÿã€‚")
        print("="*60)
        
    def _initialize_embeddings(self, use_fastembed: bool):
        """Initialize embedding model (FastEmbed or OpenAI)."""
        if use_fastembed and FASTEMBED_AVAILABLE:
            print("ğŸ”§ Initializing FastEmbed embeddings (free, local)...")
            return FastEmbedEmbeddings()
        elif OPENAI_EMBEDDINGS_AVAILABLE and self.openai_api_key:
            print("ğŸ”§ Initializing OpenAI embeddings...")
            return OpenAIEmbeddings(openai_api_key=self.openai_api_key)
        else:
            raise ValueError(
                "âŒ No embedding model available.\n"
                "   Option 1: Install FastEmbed: pip install fastembed\n"
                "   Option 2: Set OPENAI_API_KEY in .env file"
            )
    
    def _initialize_llm(self):
        """Initialize DeepSeek LLM via OpenAI-compatible API."""
        print("ğŸ”§ Initializing DeepSeek Reasoner model...")
        return ChatOpenAI(
            model="deepseek-reasoner",
            openai_api_key=self.deepseek_api_key,
            openai_api_base="https://api.deepseek.com/v1",
            temperature=0,
            max_tokens=4000,
        )


def main():
    """
    Main function to run the multi-source RAG demonstration.
    """
    print("="*60)
    print("ğŸ¤– Multi-Source RAG Agent (LangChain Educational Demo)")
    print("="*60)
    print("\nâš ï¸  This demo is designed to showcase LangChain's limitations")
    print("   in multi-source RAG scenarios.\n")
    
    try:
        # Initialize agent
        print("ğŸ”§ Initializing Multi-Source RAG Agent...")
        agent = MultiSourceRAGAgent(
            vector_db_path="./tmp/lancedb",
            use_fastembed=True,
        )
        
        # Load all data sources
        print("\nğŸ“š Step 1: Loading data sources...")
        all_documents = agent.load_all_data_sources(force_reload=False)
        
        # Check if any documents were loaded
        total_docs = sum(len(docs) for docs in all_documents.values())
        if total_docs == 0:
            print("\nâŒ No documents loaded. Please check data source files.")
            return
        
        # Create vector stores
        print("\nğŸ—„ï¸  Step 2: Creating vector stores...")
        agent.create_vector_stores(all_documents, force_reload=False)
        
        # Create retrievers
        print("\nğŸ” Step 3: Creating retrievers...")
        agent.create_retrievers(k=5)
        
        # Setup RAG chains
        print("\nâš™ï¸  Step 4: Setting up RAG chains...")
        agent.setup_rag_chains()
        
        # Run comparison
        print("\nğŸš€ Step 5: Running strategy comparison...")
        question = "EVO Nestæœºå·¢çš„æ•°æ®å­˜å‚¨æ–¹æ¡ˆä¸DJI Dockæœºåœºæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"
        
        results = agent.compare_strategies(question)
        
        # Print comparison report
        agent.print_comparison_report(results)
        
        print("\n" + "="*60)
        print("âœ¨ æ¼”ç¤ºå®Œæˆï¼")
        print("="*60)
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®:")
        print("   1. æŸ¥çœ‹è¾“å‡ºä¸­çš„LangChainå±€é™æ€§æ ‡æ³¨")
        print("   2. å¯¹æ¯”3ç§ç­–ç•¥çš„ç­”æ¡ˆè´¨é‡å·®å¼‚")
        print("   3. æ¢ç´¢ä½¿ç”¨LangGraphé‡æ„å¤šæºRAGç³»ç»Ÿ\n")
        
    except FileNotFoundError as e:
        print(f"\nâŒ æ–‡ä»¶æœªæ‰¾åˆ°é”™è¯¯: {e}")
        print("   è¯·ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨äºé¡¹ç›®æ ¹ç›®å½•:")
        for source in MultiSourceRAGAgent.DATA_SOURCES:
            print(f"   - {source['file_path']}")
        print()
        
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        print()


if __name__ == "__main__":
    main()

