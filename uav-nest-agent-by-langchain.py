"""
LangChain-based UAV Nest RAG Agent

This application uses LangChain to create a RAG (Retrieval-Augmented Generation)
system that answers questions about UAV products based on Autel Robotics product brochure.

Features:
- Local vector storage using LanceDB (no database installation required)
- FREE local embeddings using FastEmbed (no API key needed)
- DeepSeek Reasoner model for intelligent responses
- Automatic knowledge retrieval and source citation
"""

import os
from pathlib import Path
from typing import Optional
import getpass

from dotenv import load_dotenv
import lancedb

from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import LanceDB
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Try importing FastEmbed (FREE local embedder)
try:
    from langchain_community.embeddings import FastEmbedEmbeddings
    FASTEMBED_AVAILABLE = True
    print("âœ… Using FastEmbedEmbeddings (free local embedder)")
except ImportError as e:
    FASTEMBED_AVAILABLE = False
    print(f"âš ï¸  FastEmbedEmbeddings not available: {e}")
    print("   Install with: pip install fastembed")

# Try importing OpenAI embeddings as fallback
try:
    from langchain_openai import OpenAIEmbeddings
    OPENAI_EMBEDDINGS_AVAILABLE = True
except ImportError:
    OPENAI_EMBEDDINGS_AVAILABLE = False


# Load environment variables from .env file (optional)
load_dotenv()

# Load API keys (will be set interactively if not found)
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")


def get_api_key_interactive(key_name: str = "DEEPSEEK_API_KEY") -> str:
    """
    äº¤äº’å¼è·å– API Keyï¼Œä»¥æ˜Ÿå·å½¢å¼æ˜¾ç¤ºè¾“å…¥ã€‚
    
    Args:
        key_name: API Key çš„åç§°
        
    Returns:
        ç”¨æˆ·è¾“å…¥çš„ API Key
    """
    print(f"\nğŸ”‘ è¯·è¾“å…¥ä½ çš„ {key_name}")
    if key_name == "DEEPSEEK_API_KEY":
        print("   è·å–åœ°å€: https://platform.deepseek.com/")
    else:
        print("   è·å–åœ°å€: https://platform.openai.com/")
    
    api_key = getpass.getpass("   API Key (è¾“å…¥æ—¶ä¼šéšè—): ").strip()
    
    if not api_key:
        raise ValueError(f"âŒ {key_name} ä¸èƒ½ä¸ºç©º")
    
    print(f"âœ… {key_name} å·²æ¥æ”¶\n")
    return api_key


class UAVNestAgent:
    """
    A RAG agent that answers questions about UAV products based on Autel Robotics product brochure.
    """
    
    def __init__(
        self,
        pdf_path: str,
        vector_db_path: str = "./tmp/lancedb",
        table_name: str = "uav_nest",
        use_fastembed: bool = True,
        deepseek_api_key: Optional[str] = None,
        openai_api_key: Optional[str] = None,
    ):
        """
        Initialize the UAV Nest Agent.
        
        Args:
            pdf_path: Path to the Autel Robotics product brochure PDF
            vector_db_path: Path to store the LanceDB vector database
            table_name: Name of the LanceDB table
            use_fastembed: Whether to use FastEmbed (free) or OpenAI embeddings
            deepseek_api_key: DeepSeek API Key (optional, will prompt if not provided)
            openai_api_key: OpenAI API Key (optional, only needed if not using FastEmbed)
        """
        self.pdf_path = pdf_path
        self.vector_db_path = vector_db_path
        self.table_name = table_name
        
        # Get DeepSeek API Key (from parameter, environment, or interactive input)
        self.deepseek_api_key = deepseek_api_key or DEEPSEEK_API_KEY
        if not self.deepseek_api_key:
            self.deepseek_api_key = get_api_key_interactive("DEEPSEEK_API_KEY")
        
        # Get OpenAI API Key if needed
        self.openai_api_key = openai_api_key or OPENAI_API_KEY
        if not use_fastembed and not self.openai_api_key:
            self.openai_api_key = get_api_key_interactive("OPENAI_API_KEY")
        
        # Initialize embeddings
        self.embeddings = self._initialize_embeddings(use_fastembed)
        
        # Initialize LLM (DeepSeek)
        self.llm = self._initialize_llm()
        
        # Initialize vector store
        self.vector_store = None
        self.retriever = None
        self.rag_chain = None
        
    def _initialize_embeddings(self, use_fastembed: bool):
        """Initialize embedding model (FastEmbed or OpenAI)."""
        if use_fastembed and FASTEMBED_AVAILABLE:
            print("ğŸ”§ Initializing FastEmbed embeddings (free, local)...")
            # Using default FastEmbed model (same as agno for consistency)
            # The default model works well for both English and Chinese text
            return FastEmbedEmbeddings()  # Use default model
        elif OPENAI_EMBEDDINGS_AVAILABLE and self.openai_api_key:
            print("ğŸ”§ Initializing OpenAI embeddings...")
            return OpenAIEmbeddings(openai_api_key=self.openai_api_key)
        else:
            raise ValueError(
                "âŒ No embedding model available.\n"
                "   Option 1: Install FastEmbed: pip install fastembed\n"
                "   Option 2: Set OPENAI_API_KEY in .env file"
            )
    
    def _initialize_llm(self):
        """Initialize DeepSeek LLM via OpenAI-compatible API."""
        print("ğŸ”§ Initializing DeepSeek Reasoner model...")
        return ChatOpenAI(
            model="deepseek-reasoner",
            openai_api_key=self.deepseek_api_key,
            openai_api_base="https://api.deepseek.com/v1",
            temperature=0,  # More deterministic for factual answers
            max_tokens=4000,
        )
    
    def load_and_index_pdf(self, force_reload: bool = False):
        """
        Load PDF, split into chunks, and create vector store.
        
        Args:
            force_reload: If True, reload even if vector store exists
        """
        vector_db_dir = Path(self.vector_db_path)
        
        # Check if vector store already exists and has the table
        table_exists = False
        if vector_db_dir.exists() and not force_reload:
            try:
                db = lancedb.connect(self.vector_db_path)
                table_exists = self.table_name in db.table_names()
                if table_exists:
                    table = db.open_table(self.table_name)
                    row_count = table.count_rows()
                    if row_count > 0:
                        print(f"ğŸ“‚ Loading existing vector store from {self.vector_db_path}...")
                        print(f"   Table '{self.table_name}' has {row_count} rows")
                        self._load_existing_vector_store()
                        return
                    else:
                        print(f"âš ï¸  Table '{self.table_name}' exists but is empty, rebuilding...")
                        table_exists = False
                else:
                    print(f"âš ï¸  Table '{self.table_name}' not found, creating new index...")
            except Exception as e:
                print(f"âš ï¸  Error checking existing table: {e}")
                print(f"   Creating new index...")
                table_exists = False
        
        if force_reload and vector_db_dir.exists():
            print(f"ğŸ”„ Force reload requested, recreating vector store...")
        
        print(f"ğŸ“„ Loading PDF from {self.pdf_path}...")
        
        # Check if PDF exists
        if not Path(self.pdf_path).exists():
            raise FileNotFoundError(
                f"âŒ PDF file not found: {self.pdf_path}\n"
                f"   Please place your Autel Robotics product brochure PDF at this location."
            )
        
        # Load PDF
        loader = PyPDFLoader(self.pdf_path)
        documents = loader.load()
        
        print(f"âœ… Loaded {len(documents)} pages from PDF")
        
        # Split documents into chunks
        print("âœ‚ï¸  Splitting documents into chunks...")
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1500,  # Larger chunks to capture complete policy sections
            chunk_overlap=300,  # More overlap to ensure context continuity
            length_function=len,
            separators=["\n\n", "\n", "ã€‚", "ï¼›", " ", ""]  # Add Chinese punctuation separators
        )
        chunks = text_splitter.split_documents(documents)
        
        print(f"âœ… Created {len(chunks)} text chunks")
        
        # Create vector store
        print("ğŸ—„ï¸  Creating vector store (this may take a moment)...")
        
        # Create LanceDB connection
        db = lancedb.connect(self.vector_db_path)
        
        # Create vector store from documents
        self.vector_store = LanceDB.from_documents(
            documents=chunks,
            embedding=self.embeddings,
            connection=db,
            table_name=self.table_name,
        )
        
        print(f"âœ… Vector store created at {self.vector_db_path}")
        
        # Create retriever
        self._setup_retriever()
    
    def _load_existing_vector_store(self):
        """Load an existing vector store."""
        db = lancedb.connect(self.vector_db_path)
        
        self.vector_store = LanceDB(
            connection=db,
            embedding=self.embeddings,
            table_name=self.table_name,
        )
        
        print("âœ… Vector store loaded successfully")
        
        # Create retriever
        self._setup_retriever()
    
    def _setup_retriever(self):
        """Setup retriever and RAG chain."""
        # Create retriever
        self.retriever = self.vector_store.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 20}  # Retrieve top 20 chunks for better coverage (agno uses 10 by default)
        )
        
        # Create RAG prompt template (æ”¯æŒä¸­æ–‡å›ç­”)
        template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ— äººæœºäº§å“é—®ç­”åŠ©æ‰‹ï¼ŒåŸºäº Autel Robotics äº§å“æ‰‹å†Œå†…å®¹å›ç­”å…³äºæ— äººæœºäº§å“çš„é—®é¢˜ã€‚

è¯·æ ¹æ®ä»¥ä¸‹äº§å“æ‰‹å†Œçš„ä¸Šä¸‹æ–‡æ¥å›ç­”é—®é¢˜ã€‚å¦‚æœåœ¨æä¾›çš„ä¸Šä¸‹æ–‡ä¸­æ‰¾ä¸åˆ°ç­”æ¡ˆï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚

å›ç­”æ—¶è¯·éµå¾ªä»¥ä¸‹è¦æ±‚ï¼š
1. ä½¿ç”¨ä¸­æ–‡å›ç­”
2. ç¡®ä¿ç­”æ¡ˆä¸äº§å“æ‰‹å†Œå†…å®¹ä¸€è‡´
3. å°½å¯èƒ½å¼•ç”¨æ¥æºé¡µç 
4. å›ç­”è¦å…·ä½“ã€å‡†ç¡®

äº§å“æ‰‹å†Œç›¸å…³å†…å®¹ï¼š
{context}

é—®é¢˜ï¼š{question}

å›ç­”ï¼ˆè¯·å…·ä½“è¯´æ˜å¹¶å¼•ç”¨æ¥æºï¼‰ï¼š"""

        prompt = ChatPromptTemplate.from_template(template)
        
        # Create RAG chain
        def format_docs(docs):
            """Format retrieved documents for the prompt."""
            formatted = []
            for doc in docs:
                page = doc.metadata.get('page', 'unknown')
                content = doc.page_content
                formatted.append(f"[Page {page + 1}]\n{content}")
            return "\n\n".join(formatted)
        
        self.rag_chain = (
            {
                "context": self.retriever | format_docs,
                "question": RunnablePassthrough()
            }
            | prompt
            | self.llm
            | StrOutputParser()
        )
        
        print("âœ… RAG chain configured successfully")
    
    def ask(self, question: str, stream: bool = False) -> str:
        """
        Ask a question about the UAV products.
        
        Args:
            question: Question to ask
            stream: Whether to stream the response
            
        Returns:
            Answer from the agent
        """
        if not self.rag_chain:
            raise RuntimeError(
                "âŒ RAG chain not initialized. Call load_and_index_pdf() first."
            )
        
        print(f"\n{'='*60}")
        print(f"â“ Question: {question}")
        print(f"{'='*60}\n")
        
        if stream:
            print("ğŸ’¬ Answer:\n")
            full_response = ""
            for chunk in self.rag_chain.stream(question):
                print(chunk, end="", flush=True)
                full_response += chunk
            print("\n")
            return full_response
        else:
            response = self.rag_chain.invoke(question)
            print(f"ğŸ’¬ Answer:\n{response}\n")
            return response


def main():
    """Main function to demonstrate the agent."""
    
    # Configuration
    PDF_PATH = "./Autel-Robotics-Products-Brochure.pdf"
    VECTOR_DB_PATH = "./tmp/lancedb"
    
    print("="*60)
    print("ğŸ¤– UAV Nest Agent (LangChain)")
    print("="*60)
    print()
    
    try:
        # Initialize agent
        agent = UAVNestAgent(
            pdf_path=PDF_PATH,
            vector_db_path=VECTOR_DB_PATH,
            use_fastembed=True,  # Use free local embeddings
        )
        
        # Load and index PDF (force reload to ensure using correct embeddings)
        # Note: Set to False after first successful run to skip re-indexing
        agent.load_and_index_pdf(force_reload=True)
        
        print("\n" + "="*60)
        print("ğŸš€ Agent ready! Starting demo queries...")
        print("="*60)
        
        # Demo 1: å…³äº Autel äº§å“çš„æ ¸å¿ƒé—®é¢˜
        print("\nğŸ“‹ æ¼”ç¤ºé—®é¢˜ 1: å…³äºäº§å“æŠ€æœ¯è§„æ ¼")
        print("-" * 60)
        agent.ask(
            "EVO Nestäº§å“çš„ä¸»è¦æŠ€æœ¯è§„æ ¼æ˜¯ä»€ä¹ˆï¼Ÿ",
            stream=True
        )
        
        # è¯¢é—®æ˜¯å¦ç»§ç»­æ¼”ç¤º
        print("\n" + "-" * 60)
        try:
            response = input("\næŒ‰ Enter ç»§ç»­ç¬¬äºŒä¸ªæ¼”ç¤ºé—®é¢˜ï¼Œæˆ–è¾“å…¥ 'q' é€€å‡º: ").strip()
            if response.lower() == 'q':
                print("\nğŸ‘‹ æ¼”ç¤ºç»“æŸ\n")
                return
        except (KeyboardInterrupt, EOFError):
            print("\n\nğŸ‘‹ æ¼”ç¤ºç»“æŸ\n")
            return
        
        # Demo 2: å¦ä¸€ä¸ªå¸¸è§é—®é¢˜
        print("\nğŸ“‹ æ¼”ç¤ºé—®é¢˜ 2: å…³äº Autel äº§å“ç‰¹æ€§")
        print("-" * 60)
        agent.ask(
            "Autel Robotics æœ‰å“ªäº›ä¸»è¦çš„æ— äººæœºäº§å“ï¼Ÿ",
            stream=True
        )
        
        print("\n" + "="*60)
        print("âœ¨ æ¼”ç¤ºå®Œæˆï¼")
        print("="*60)
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®:")
        print("   1. ä¿®æ”¹ä»£ç å°è¯•ä¸åŒé—®é¢˜")
        print("   2. æ¢ç´¢æ›´å¤šäº§å“ç‰¹æ€§\n")
        
    except Exception as e:
        print(f"\nâŒ Error: {e}\n")
        raise


if __name__ == "__main__":
    main()

